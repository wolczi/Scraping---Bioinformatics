{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import wget\n",
    "import bs4\n",
    "import os\n",
    "import urllib.parse\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import fnmatch\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "from Bio import Entrez, SeqIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4164b",
   "metadata": {},
   "source": [
    "__ZADANIE 1__\n",
    "\n",
    "Z bazy danych czasopisma BMC Systems Biology ze strony (https://bmcsystbiol.biomedcentral.com/articles) pobierz i\n",
    "zapisz do folderu BaseArticles informacje, które obejmują jakiś miesiąc danego roku, np. grudzień 2018:\n",
    "    \n",
    "a) jeden plik tekstowy (nazwa pliku = nazwa miesiąca) zawierający kolejno informacje (tytuł artykułu, autorzy,\n",
    "zawartość abstraktu) z artykułów obejmujących zadany okres <br>\n",
    "b) jeżeli w tytule artykułu wystąpiło słowo ’network lub networks’ zapisz również plik pdf artykułu do folderu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Odczyt kodu HTML ##\n",
    "url = 'https://bmcsystbiol.biomedcentral.com/articles?tab=keyword&searchType=journalSearch&sort=PubDate&volume=12&page=1'\n",
    "codeHTML = requests.get(url, verify=True).text # odczyt zawartości strony (HTML)\n",
    "# #print(codeHTML)\n",
    "soup = bs4.BeautifulSoup(codeHTML, 'html.parser') # tworzenie obiektu BeautifulSoup z zachowaniem struktury kodu strony\n",
    "# #print(soup) # wyświetlenie kodu z zachowaniem struktury\n",
    "\n",
    "f1 = open(\"grudzien.txt\",\"w+\", encoding=\"utf-8\") #zapis z miesiąca grudnia\n",
    "\n",
    "articles_list = list(soup.findAll('article', attrs = {'class' : 'c-listing'}))\n",
    "#print(\"ARTYKULY Z GRUDNIA: \" + str(len(articles_list)))\n",
    "print(\"(INFO) Zapisuje dane do pliku grudzien.txt i pobieram pdf'y\")\n",
    "\n",
    "for article in articles_list:\n",
    "    if (article.find(\"span\", {\"itemprop\": \"datePublished\"}).text.endswith(\"December 2018\"))==True:\n",
    "        url= \"https://bmcsystbiol.biomedcentral.com\" + str(article.find(\"a\").get('href'))\n",
    "        f1.write(\"LINK DO ARTYKULU ---> \" + url + \"\\n\")\n",
    "        ### przejście na strone danego artykułu\n",
    "        codeHTML = requests.get(url, verify=True).text\n",
    "        soup = bs4.BeautifulSoup(codeHTML, 'html.parser')\n",
    "        title=soup.find(\"meta\", {\"name\": \"dc.title\"}).get(\"content\")\n",
    "        x = re.findall(\"NETWORK\", title.upper()) # szukanie artykułów z \"network\" w stringu\n",
    "        if x: # jeśli \"network\" w str to pobiera pdf\n",
    "            article_page = soup.find('div', attrs={'class': 'c-pdf-download u-clear-both'})\n",
    "            pdf = \"https:\" + str(article_page.find(\"a\").get('href'))\n",
    "            f1.write(\"LINK DO PDFA ---> \" + pdf + \"\\n\")\n",
    "            response = requests.get(pdf)\n",
    "            open(\"C:\\\\Users\\\\Przemek\\\\PycharmProjects\\\\scraping\\\\zapis\\\\\" + pdf.rsplit('/', 1)[-1], 'wb').write(response.content)\n",
    "        f1.write(\"TYTUL ---> \" + title + \"\\n\")\n",
    "        f1.write(\"AUTORZY ---> \")\n",
    "        for authors in soup.findAll(\"meta\", {\"name\": \"dc.creator\"}):\n",
    "            f1.write(authors.get(\"content\") + \"    \")\n",
    "        f1.write(\"\\nZAWARTOSC ABSTRACT ---> \" + soup.find(\"meta\", {\"property\": \"og:description\"}).get(\"content\") + \"\\n\\n\\n\")\n",
    "\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84a78d",
   "metadata": {},
   "source": [
    "__ZADANIE 2__\n",
    "\n",
    "Napisz program, który umożliwi użytkownikowi pobranie określonych danych z medycznej bazy danych emedicine\n",
    "(https://emedicine.medscape.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39588b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://emedicine.medscape.com/'\n",
    "codeHTML = requests.get(url, verify=True).text\n",
    "soup = bs4.BeautifulSoup(codeHTML, 'html.parser')\n",
    "\n",
    "categories = soup.find('div', attrs = {'class' : 'browse-medicine'})\n",
    "categories_list=list(categories.findAll(\"a\"))\n",
    "\n",
    "### wyświetlanie 30-tu kategorii dostępnych w BROWSE BY SPECIALITY: MEDICINE\n",
    "i=0\n",
    "for category in categories_list:\n",
    "    categories_list[i]=category.text\n",
    "    i+=1\n",
    "\n",
    "categories_list={index+1:value for index,value in enumerate(categories_list)}\n",
    "print(categories_list)\n",
    "\n",
    "### wybieranie do której kategorii mamy wejść\n",
    "choice=categories_list.get(int(input('Którą kategorie wybierasz?: ')))\n",
    "print(choice)\n",
    "\n",
    "### przechodzenie na strone konkretnej kategorii\n",
    "for category in soup.findAll(\"a\"):\n",
    "    if(category.text==choice):\n",
    "        if(category.get(\"href\").startswith(\"http\")):\n",
    "            subpage=category.get(\"href\")\n",
    "        else:\n",
    "            subpage=\"https://emedicine.medscape.com\" + str(category.get(\"href\"))\n",
    "\n",
    "url = subpage\n",
    "codeHTML = requests.get(url, verify=True).text\n",
    "soup = bs4.BeautifulSoup(codeHTML, 'html.parser')\n",
    "\n",
    "### wyświetlanie podkategorii z wcześniej wybranej kategorii\n",
    "topics=soup.findAll(\"div\", {\"class\": \"topic-head\"})\n",
    "i=0\n",
    "for topic in topics:\n",
    "    topics[i]=topic.text\n",
    "    i+=1\n",
    "\n",
    "topics_list={index+1:value for index,value in enumerate(topics)}\n",
    "print(topics_list)\n",
    "\n",
    "### wybieranie podkategorii z którą będziemy pracować\n",
    "choice=topics_list.get(int(input('Którą podkategorie wybierasz?: ')))\n",
    "print(choice)\n",
    "\n",
    "values = topics_list.values()\n",
    "values_list = list(values)\n",
    "index = values_list.index(choice)\n",
    "\n",
    "\n",
    "subcategories=soup.findAll('div', attrs = {\"class\": \"topic-section\"})[index]\n",
    "\n",
    "### wchodzenie po kolei do każdej podkategorii\n",
    "subcategories=subcategories.findAll(\"a\")\n",
    "for subcategory in subcategories:\n",
    "    link=subcategory.get(\"href\")\n",
    "    #print(link)\n",
    "    codeHTML2 = requests.get(link, verify=True).text\n",
    "    soup2 = bs4.BeautifulSoup(codeHTML2, 'html.parser')\n",
    "\n",
    "    title=soup2.find('span', attrs={\"class\": \"nav-title-name\"})\n",
    "    title=title.text\n",
    "    #print(title)\n",
    "\n",
    "    ## pobieranie zdjęć z podkategorii (jeśli są)\n",
    "    try:\n",
    "        image = soup2.find('div', attrs={\"class\": \"imgWrapper\"}).get(\"data-rel\")\n",
    "        image = \"http:\" + image\n",
    "        #print(image)\n",
    "        img = Image.open(requests.get(image, stream=True).raw)\n",
    "        img.save(\"C:\\\\Users\\\\Przemek\\\\PycharmProjects\\\\scraping\\\\zdjecia\\\\\" + image.split(\"/\")[-1], 'PNG')\n",
    "    except:\n",
    "       print(\"!!! An exception occurred\")\n",
    "\n",
    "    ## zapisywanie tekstu z background (jeśli jest)\n",
    "    try:\n",
    "        menu_list = soup2.find('div', attrs={\"id\": \"menuWrapper\"})\n",
    "        menu_list = menu_list.findAll(\"a\")\n",
    "        for menu in menu_list:\n",
    "            if (menu.text == \"Background\"):\n",
    "                f1 = open('C:\\\\Users\\\\pwolc\\\\PycharmProjects\\\\lab8_bioinf\\\\venv\\\\BazaTekst\\\\' + title + '.txt', \"w+\",\n",
    "                          encoding=\"utf-8\")\n",
    "                content_nr = menu.get(\"href\")[1:]\n",
    "                # print(content_nr)\n",
    "                content_nr = \"content_\" + content_nr\n",
    "                backgroundDiv = soup2.find('div', {\"id\": content_nr})\n",
    "                if (backgroundDiv is not None):\n",
    "                    for paragraph in backgroundDiv.find_all('p'):\n",
    "                        for sup in paragraph.findAll('sup'):\n",
    "                            sup.extract()\n",
    "                        f1.write(paragraph.text + \"\\n\")\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "\n",
    "print(\"(INFO) Pobrałem artykuły z wybranej podkategorii do folderu BazaTekst\")\n",
    "print(\"(INFO) Pobrałem zdjęcia z artykułów do folderu BazaImage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed249e90",
   "metadata": {},
   "source": [
    "__ZADANIE 3__ \n",
    "\n",
    "Połącz się z Entrez - systemem wyszukiwania informacji w NCBI. <br>\n",
    "Sprawdż ile publikacji w bazie PubMed znajduje się na temat 'covid-19' (parametr term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30042c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"pwolczacki@gmail.com\"\n",
    "info = Entrez.esearch(db = \"pubmed\",term = \"covid-19\")  # m.in przedstawia listę identyfikatorów UID pasujących do zapytania tekstowego\n",
    "print(\"\\n########## ZAD 1, LAB 9\\nPublikacje w bazie PubMed na temat 'covid-19': \" + Entrez.read(info)['Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee806fc4",
   "metadata": {},
   "source": [
    "__ZADANIE 4__\n",
    "\n",
    "Połącz się z Entrez - systemem wyszukiwania informacji w NCBI. <br>\n",
    "W bazie danych nucleotide wyszukaj informacje dla kwerendy: 'covid' <br>\n",
    "a) wypisz wszystkie UID  dla w/w kwerendy (key: IdList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"pwolczacki@gmail.com\"\n",
    "info = Entrez.esearch(db = \"nucleotide\",term = \"covid\")\n",
    "record = Entrez.read(info)\n",
    "print(\"\\n########## ZAD 2, LAB 9\\na) Wszystkie UID dla kwerendy 'covid' w bazie Nucleotide: \" + str(record['IdList']))\n",
    "\n",
    "UID=record['IdList'][2]\n",
    "#print(type(UID))\n",
    "info = Entrez.efetch(db = \"nucleotide\", id = str(UID), rettype=\"gb\", retmode=\"xml\")\n",
    "#record = Entrez.read(info)\n",
    "#print(record)\n",
    "records = Entrez.parse(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b0570",
   "metadata": {},
   "source": [
    "b) dla 3-go znalezionego UID odczytaj (użyj funkcji efetch z parametrem retmode=\"xml\") <br>\n",
    "dodatkowe informacje z bazy \"nucleotide\": <br>\n",
    "(i) nazwę tego biomarkera/cząstki <br>\n",
    "(ii) z jakiego organizmu były pobrane próbki do badań"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in records:\n",
    "    print(\"b) Nazwa biomarkera/cząstki: \" + record['GBSeq_locus'])\n",
    "    print(\"Próbki do badań pobrane z: \" + record['GBSeq_organism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac505c21",
   "metadata": {},
   "source": [
    "__ZADANIE 5__\n",
    "\n",
    "Połącz się z Entrez - systemem wyszukiwania informacji w NCBI. <br>\n",
    "Wyszukaj informacje o sekwencji  id = 'NC_045512' <br>\n",
    "odpowiedz na pytania: <br>\n",
    "(a) Jak długa jest sekwencja nukleotydowa? <br>\n",
    "(b) Od jakiego organizmu pochodzi? <br>\n",
    "(c) Kiedy opublikowano pierwsze wyniki? <br>\n",
    "(d) Czy ta sekwencja była kiedykolwiek poprawiana/aktualizowana? (kolejne wersje?) <br>\n",
    "(e) Kto jest pierwszym autorem badań sekwencji? <br>\n",
    "(f) Gdzie (w jakich czasopismach) opublikowano wyniki badań tej sekwencji po zgłoszeniu w bazie (wszystkie publikacje) <br>\n",
    "(g) Jaką część sekwencji opisano jako gen (ang. gene)? Jak długi jest ten fragment? I jaką nazwę genu <br>\n",
    "przypisano temu fragmentowi? <br>\n",
    "(h) Przełącz się na widok w formacie FASTA. Spróbuj rozszyfrować, co wpisano jako opis sekwencji. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"pwolczacki@gmail.com\"\n",
    "info = Entrez.efetch(db=\"nucleotide\", id=\"NC_045512\", rettype=\"gb\", retmode=\"xml\")\n",
    "#record = Entrez.read(info)\n",
    "#print(record)\n",
    "#print(json.dumps(record, sort_keys=True, indent=4))\n",
    "\n",
    "records = Entrez.parse(info)\n",
    "\n",
    "for record in records:\n",
    "    print(\"\\n########## ZAD 3, LAB 9\\na) Długość sekwencji nukleotydowej: \" + record['GBSeq_length'])\n",
    "    print(\"b) Nazwa organizmu: \" + record['GBSeq_organism'])\n",
    "    print(\"c) Pierwsze wyniki opublikowano: \" + record['GBSeq_create-date'])\n",
    "    print(\"d) Czy aktualizowano?: \" +\n",
    "          (\"Tak\" if record['GBSeq_update-date'] != record['GBSeq_create-date'] else \"Nie\"))\n",
    "    print(\"e) Pierwsi autorzy: \" +\n",
    "          str((record['GBSeq_references'][0]['GBReference_authors'])))\n",
    "    print(\"f) Publikacje badań: \")\n",
    "    for ref in record['GBSeq_references']:\n",
    "        print(\"    \" + ref['GBReference_journal'])\n",
    "    print(\"g) \")\n",
    "    for ref in record['GBSeq_feature-table']:\n",
    "        if (ref['GBFeature_key'] == 'gene'):\n",
    "            print(\"Długość fragmentu części sekwencji opisanej jako gen (ang. gene): \", ref['GBFeature_location'])\n",
    "            print(\"Nazwa genu: \", ref['GBFeature_quals'][0]['GBQualifier_value'])\n",
    "\n",
    "print(\"\\n h) Opis w formacie .FASTA:\\n\" + SeqIO.read(Entrez.efetch(\n",
    "    db=\"nucleotide\", id=\"NC_045512\", rettype=\"fasta\"), \"fasta\").description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
